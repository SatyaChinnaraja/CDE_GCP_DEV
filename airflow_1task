#!/usr/bin/env python
# coding: utf-8

# In[4]:


import airflow 
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator
from datetime import timedelta
from google.cloud import storage

default_args = {
    'start_date': airflow.utils.dates.days_ago(0),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

bq_load = "bq load --source"



def create_bucket_class_location():
    """
    Create a new bucket in the US region with the coldline storage
    class
    """
    bucket_name = "satya-airflow-batch9-04112022"

    storage_client = storage.Client()

    bucket = storage_client.bucket(bucket_name)
    bucket.storage_class = "COLDLINE"
    new_bucket = storage_client.create_bucket(bucket, location="us")

    print(
        "Created bucket {} in {} with storage class {}".format(
            new_bucket.name, new_bucket.location, new_bucket.storage_class
        )
    )
    

dag = DAG(
    'createbucket',
    default_args=default_args,
    description='liveness monitoring dag',
    schedule_interval=None,
    dagrun_timeout=timedelta(minutes=20))

# priority_weight has type int in Airflow DB, uses the maximum

t1 = PythonOperator(
    task_id='echo',
    python_callable=create_bucket_class_location,
    dag=dag,
    depends_on_past=False,
    priority_weight=2**31 - 1,
    do_xcom_push=False)
    
    
    

# In[ ]:




